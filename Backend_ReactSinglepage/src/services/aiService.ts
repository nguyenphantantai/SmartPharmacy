import { config } from '../config/index.js';
import { systemPrompt, systemInstructionGemini } from './aiPrompts.js';

// Lazy load AI clients to avoid errors if packages not installed
let openaiClient: any = null;
let geminiClient: any = null;

// Export initialization function for server startup
export async function initializeAIClients() {
  await initializeClients();
}

// Initialize clients on first use
async function initializeClients() {
  // Initialize OpenAI
  if (!openaiClient && process.env.OPENAI_API_KEY) {
    try {
      // Dynamic import to avoid errors if package not installed
      const openaiModule = await import('openai');
      const OpenAI = (openaiModule as any).default || openaiModule;
      openaiClient = new OpenAI({
        apiKey: process.env.OPENAI_API_KEY,
      });
      console.log('‚úÖ OpenAI initialized');
    } catch (error) {
      console.log('‚ö†Ô∏è OpenAI package not installed');
    }
  }

  // Initialize Gemini
  if (!geminiClient && process.env.GEMINI_API_KEY) {
    try {
      const { GoogleGenerativeAI } = await import('@google/generative-ai');
      geminiClient = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);
      // Default to gemini-2.5-flash (stable and fast), user can override with GEMINI_MODEL
      const modelName = process.env.GEMINI_MODEL || 'gemini-2.5-flash';
      console.log(`‚úÖ Google Gemini AI initialized (Model: ${modelName})`);
    } catch (error: any) {
      console.log('‚ö†Ô∏è Google Gemini package not installed or error:', error.message);
      console.log('   Run: npm install @google/generative-ai');
      console.log('   Add GEMINI_API_KEY to .env file');
    }
  } else if (!process.env.GEMINI_API_KEY) {
    console.log('‚ÑπÔ∏è GEMINI_API_KEY not found in environment variables');
  }
}

// Alternative: Use other AI services
// - Google Gemini API
// - Anthropic Claude API
// - Local LLM (Ollama, LM Studio)
// - Vietnamese LLM (VinAI, FPT AI)

interface AIChatOptions {
  userMessage: string;
  conversationHistory: Array<{ role: 'user' | 'assistant'; content: string }>;
  context?: {
    medicines?: any[];
    userHistory?: any[];
    symptoms?: string[];
  };
}

/**
 * Generate AI response using OpenAI GPT
 * Fallback to rule-based system if API key not configured
 */
export async function generateAIResponseWithLLM(options: AIChatOptions): Promise<string> {
  const { userMessage, conversationHistory, context } = options;

  // Initialize if not already done
  await initializeClients();

  // If OpenAI is not configured, return null to use rule-based system
  if (!openaiClient) {
    return null as any; // Signal to use fallback
  }

  try {
    // Build context information
    let contextInfo = '';
    if (context?.medicines && context.medicines.length > 0) {
      contextInfo += `\n\nTh√¥ng tin thu·ªëc c√≥ s·∫µn trong h·ªá th·ªëng (g·ª£i √Ω t·ªëi ƒëa 3 thu·ªëc):\n`;
      // Limit to 3 medicines max to reduce tokens
      context.medicines.slice(0, 3).forEach((med, idx) => {
        contextInfo += `${idx + 1}. ${med.name}`;
        // QUAN TR·ªåNG: Ch·ªâ hi·ªÉn th·ªã c√¥ng d·ª•ng (indication), KH√îNG hi·ªÉn th·ªã h√†m l∆∞·ª£ng ·ªü ƒë√¢y
        if (med.indication) {
          // Truncate long indications
          const shortIndication = med.indication.length > 200 
            ? med.indication.substring(0, 200) + '...' 
            : med.indication;
          contextInfo += `\n   - T√°c d·ª•ng: ${shortIndication}`;
        }
        if (med.strength) {
          contextInfo += `\n   - H√†m l∆∞·ª£ng: ${med.strength}`;
        }
        if (med.price) {
          contextInfo += `\n   - Gi√°: ${med.price.toLocaleString('vi-VN')}ƒë`;
        }
        if (med.unit) {
          contextInfo += `\n   - Quy c√°ch: ${med.unit}`;
        }
        contextInfo += '\n';
      });
      contextInfo += `\nL∆ØU √ù: Khi g·ª£i √Ω thu·ªëc, b·∫°n PH·∫¢I s·ª≠ d·ª•ng tr∆∞·ªùng "T√°c d·ª•ng" (kh√¥ng ph·∫£i h√†m l∆∞·ª£ng) trong ph·∫ßn m√¥ t·∫£ c√¥ng d·ª•ng c·ªßa thu·ªëc.\n`;
    }

    if (context?.symptoms && context.symptoms.length > 0) {
      contextInfo += `\nTri·ªáu ch·ª©ng ng∆∞·ªùi d√πng ƒë√£ ƒë·ªÅ c·∫≠p: ${context.symptoms.join(', ')}\n`;
      contextInfo += `H√£y ch·ªâ g·ª£i √Ω thu·ªëc PH√ô H·ª¢P v·ªõi c√°c tri·ªáu ch·ª©ng n√†y.\n`;
    }

    // Build messages for OpenAI
    const messages: any[] = [
      {
        role: 'system',
        content: systemPrompt + contextInfo
      },
      ...conversationHistory.map(msg => ({
        role: msg.role === 'user' ? 'user' : 'assistant',
        content: msg.content
      })),
      {
        role: 'user',
        content: userMessage
      }
    ];

    // Call OpenAI API
    const completion = await openaiClient.chat.completions.create({
      model: process.env.OPENAI_MODEL || 'gpt-4o-mini', // Use gpt-4o-mini for cost efficiency, or gpt-4o for better quality
      messages: messages,
      temperature: 0.7,
      max_tokens: 1000,
      top_p: 1,
      frequency_penalty: 0,
      presence_penalty: 0,
    });

    const aiResponse = completion.choices[0]?.message?.content || '';
    return aiResponse;

  } catch (error) {
    console.error('Error calling OpenAI API:', error);
    // Fallback to rule-based system on error
    return null as any;
  }
}

/**
 * Generate AI response using Google Gemini API
 * Free tier: 15 requests per minute, 1500 requests per day
 */
export async function generateAIResponseWithGemini(options: AIChatOptions): Promise<string> {
  const { userMessage, conversationHistory, context } = options;

  // Initialize if not already done
  await initializeClients();

  // If Gemini is not configured, return null to use rule-based system
  if (!geminiClient) {
    return null as any; // Signal to use fallback
  }

  try {
    // Get model (default: gemini-2.5-flash for stable and fast API)
    // Available models: gemini-2.5-flash, gemini-2.5-pro, gemini-2.0-flash, gemini-flash-latest
    // Note: Older models (gemini-pro, gemini-1.5-flash) are deprecated
    let modelName: string = process.env.GEMINI_MODEL || 'gemini-2.5-flash';
    
    // Map old model names to new ones
    const modelMapping: { [key: string]: string } = {
      'gemini-pro': 'gemini-pro-latest',
      'gemini-1.5-flash': 'gemini-2.5-flash',
      'gemini-1.5-pro': 'gemini-2.5-pro',
      'gemini-1.5-flash-latest': 'gemini-2.5-flash'
    };
    
    if (modelName && modelMapping[modelName]) {
      modelName = modelMapping[modelName];
    }
    
    const model = geminiClient.getGenerativeModel({ model: modelName });

    const systemInstruction = systemInstructionGemini;

    // Build context information
    let contextInfo = '';
    if (context?.medicines && context.medicines.length > 0) {
      contextInfo += `\n\n=== TH√îNG TIN THU·ªêC C√ì S·∫¥N TRONG H·ªÜ TH·ªêNG ===\n`;
      contextInfo += `QUAN TR·ªåNG: Danh s√°ch thu·ªëc d∆∞·ªõi ƒë√¢y ƒê√É ƒê∆Ø·ª¢C L·ªåC v√† CH·ªà CH·ª®A THU·ªêC PH√ô H·ª¢P v·ªõi y√™u c·∫ßu c·ªßa ng∆∞·ªùi d√πng.\n`;
      contextInfo += `B·∫°n PH·∫¢I ch·ªâ g·ª£i √Ω c√°c thu·ªëc trong danh s√°ch n√†y, KH√îNG ƒë∆∞·ª£c g·ª£i √Ω thu·ªëc kh√°c.\n`;
      contextInfo += `Ch·ªâ g·ª£i √Ω 3-5 thu·ªëc ph√π h·ª£p nh·∫•t t·ª´ danh s√°ch n√†y.\n\n`;
      
      // Limit to 5 medicines max, prioritize by relevance
      context.medicines.slice(0, 3).forEach((med, idx) => {
        contextInfo += `${idx + 1}. **${med.name}**\n`;
        // QUAN TR·ªåNG: Ch·ªâ hi·ªÉn th·ªã c√¥ng d·ª•ng (indication), KH√îNG hi·ªÉn th·ªã h√†m l∆∞·ª£ng ·ªü ƒë√¢y
        if (med.indication) {
          // Truncate long indications
          const shortIndication = med.indication.length > 200 
            ? med.indication.substring(0, 200) + '...' 
            : med.indication;
          contextInfo += `   - T√°c d·ª•ng: ${shortIndication}\n`;
        } else if (med.description) {
          const shortDesc = med.description.length > 200 
            ? med.description.substring(0, 200) + '...' 
            : med.description;
          contextInfo += `   - T√°c d·ª•ng: ${shortDesc}\n`;
        }
        if (med.strength) {
          contextInfo += `   - H√†m l∆∞·ª£ng: ${med.strength}\n`;
        }
        if (med.price) {
          contextInfo += `   - Gi√°: ${med.price.toLocaleString('vi-VN')}ƒë\n`;
        }
        if (med.unit) {
          contextInfo += `   - Quy c√°ch: ${med.unit}\n`;
        }
        if (med.stockQuantity) {
          contextInfo += `   - T·ªìn kho: ${med.stockQuantity} ${med.unit || 's·∫£n ph·∫©m'}\n`;
        }
        contextInfo += '\n';
      });
      contextInfo += `\n=== QUY T·∫ÆC QUAN TR·ªåNG ===\n`;
      contextInfo += `1. CH·ªà g·ª£i √Ω c√°c thu·ªëc trong danh s√°ch tr√™n, KH√îNG ƒë∆∞·ª£c g·ª£i √Ω thu·ªëc kh√°c.\n`;
      contextInfo += `2. Tr∆∞·ªùng "T√°c d·ª•ng" PH·∫¢I l√† m√¥ t·∫£ c√¥ng d·ª•ng (v√≠ d·ª•: "H·∫° s·ªët, gi·∫£m ƒëau nh·∫π"), KH√îNG ƒë∆∞·ª£c ghi h√†m l∆∞·ª£ng (v√≠ d·ª•: "500mg" l√† SAI).\n`;
      contextInfo += `3. N·∫øu "T√°c d·ª•ng" trong danh s√°ch ch·ªâ l√† h√†m l∆∞·ª£ng, b·∫°n PH·∫¢I t·∫°o m√¥ t·∫£ c√¥ng d·ª•ng d·ª±a tr√™n t√™n thu·ªëc.\n`;
      contextInfo += `4. Format: [S·ªë]. **[T√™n thu·ªëc]**\n   üí∞ Gi√°: [gi√°]ƒë\n   üíä T√°c d·ª•ng: [m√¥ t·∫£ c√¥ng d·ª•ng]\n   üì¶ Quy c√°ch: [quy c√°ch]\n`;
    }

    if (context?.symptoms && context.symptoms.length > 0) {
      contextInfo += `\n=== TRI·ªÜU CH·ª®NG NG∆Ø·ªúI D√ôNG ===\n`;
      contextInfo += `Ng∆∞·ªùi d√πng ƒë√£ ƒë·ªÅ c·∫≠p: ${context.symptoms.join(', ')}\n`;
      contextInfo += `Y√™u c·∫ßu g·ªëc: "${(context as any).userQuery || userMessage}"\n`;
      contextInfo += `B·∫°n PH·∫¢I ch·ªâ g·ª£i √Ω thu·ªëc PH√ô H·ª¢P v·ªõi tri·ªáu ch·ª©ng n√†y t·ª´ danh s√°ch thu·ªëc ƒë√£ ƒë∆∞·ª£c l·ªçc ·ªü tr√™n.\n`;
    }

    if (context?.userHistory && context.userHistory.length > 0) {
      contextInfo += `\nL·ªãch s·ª≠ mua h√†ng c·ªßa ng∆∞·ªùi d√πng:\n`;
      context.userHistory.slice(0, 3).forEach((item, idx) => {
        contextInfo += `${idx + 1}. ${item.productName}\n`;
      });
    }

    // Build conversation history for Gemini
    // Gemini requires: first message must be from 'user', not 'model'
    // Format: parts array with text
    const chatHistory: any[] = [];
    
    // Filter and add conversation history
    // Skip if history starts with 'assistant' (model) - Gemini doesn't allow this
    let skipFirst = false;
    if (conversationHistory.length > 0 && conversationHistory[0]?.role === 'assistant') {
      skipFirst = true;
    }
    
    for (let i = 0; i < conversationHistory.length; i++) {
      const msg = conversationHistory[i];
      
      // Skip if message is undefined
      if (!msg) {
        continue;
      }
      
      // Skip first message if it's from assistant
      if (i === 0 && skipFirst) {
        continue;
      }
      
      if (msg.role === 'user') {
        chatHistory.push({
          role: 'user',
          parts: [{ text: msg.content }]
        });
      } else if (msg.role === 'assistant') {
        chatHistory.push({
          role: 'model',
          parts: [{ text: msg.content }]
        });
      }
    }

    // Start chat session
    // systemInstruction must be an object with parts array, not a string
    const fullSystemInstruction = systemInstructionGemini + contextInfo;
    const chat = model.startChat({
      history: chatHistory.length > 0 ? chatHistory : undefined, // Only include if not empty
      systemInstruction: {
        parts: [{ text: fullSystemInstruction }]
      },
      generationConfig: {
        temperature: 0.7,
        topK: 40,
        topP: 0.95,
        maxOutputTokens: 4096, // Increased to prevent response truncation
      },
    });

    // Send user message
    const result = await chat.sendMessage(userMessage);
    const response = await result.response;
    let aiResponse = response.text();
    
    // Check if response was truncated (ends abruptly)
    // Gemini sometimes truncates if maxOutputTokens is reached
    if (aiResponse && aiResponse.length > 0) {
      console.log(`‚úÖ Gemini response received (${aiResponse.length} characters)`);
      
      // If response seems incomplete (ends mid-sentence), log a warning
      const lastChar = aiResponse.trim().slice(-1);
      if (!['.', '!', '?', ':', ';'].includes(lastChar) && aiResponse.length > 1000) {
        console.log('‚ö†Ô∏è Response might be truncated (does not end with punctuation)');
      }
    }

    return aiResponse;

  } catch (error: any) {
    console.error('Error calling Gemini API:', error);
    
    // Handle rate limit errors
    if (error.message?.includes('429') || error.message?.includes('quota')) {
      console.log('‚ö†Ô∏è Gemini API rate limit reached, falling back to rule-based system');
    }
    
    // Fallback to rule-based system on error
    return null as any;
  }
}

/**
 * Generate AI response using Anthropic Claude API (Alternative)
 */
export async function generateAIResponseWithClaude(options: AIChatOptions): Promise<string> {
  // Implementation for Anthropic Claude API
  // Requires: npm install @anthropic-ai/sdk
  // import Anthropic from '@anthropic-ai/sdk';
  // const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY });
  // ...
  return null as any;
}

/**
 * Generate AI response using local LLM (Ollama) - Free alternative
 */
export async function generateAIResponseWithOllama(options: AIChatOptions): Promise<string> {
  try {
    const ollamaUrl = process.env.OLLAMA_URL || 'http://localhost:11434';
    const model = process.env.OLLAMA_MODEL || 'llama3.2:3b'; // or 'mistral', 'phi3', etc.

    const response = await fetch(`${ollamaUrl}/api/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: model,
        messages: [
          {
            role: 'system',
            content: 'B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ d∆∞·ª£c ph·∫©m. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, ch√≠nh x√°c v√† an to√†n.'
          },
          ...options.conversationHistory,
          { role: 'user', content: options.userMessage }
        ],
        stream: false
      })
    });

    if (!response.ok) {
      return null as any;
    }

    const data = await response.json();
    return data.message?.content || null as any;
  } catch (error) {
    console.error('Error calling Ollama:', error);
    return null as any;
  }
}

